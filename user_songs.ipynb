{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd04bba1d32cad56bd564e8db7f54bce5a3582261770c0fd88a009216e68a1efe96",
   "display_name": "Python 3.8.5 64-bit ('Aditya': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Part 1: Joining our data\n",
    "\n",
    "Our data was obtained through 3 seperate csv files, so we need to join them all together   \n",
    "We start by reading the in dataframe containing user_id, song_id, and listen counts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           user_id             song_id  count\n",
       "0         b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAKIMP12A8C130995      1\n",
       "1         b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAPDEY12A81C210A9      1\n",
       "2         b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBBMDR12A8C13253B      2\n",
       "3         b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFNSP12AF72A0E22      1\n",
       "4         b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFOVM12A58A7D494      1\n",
       "...                                            ...                 ...    ...\n",
       "48373581  b7815dbb206eb2831ce0fe040d0aa537e2e800f7  SOUHHHH12AF729E4AF      2\n",
       "48373582  b7815dbb206eb2831ce0fe040d0aa537e2e800f7  SOUJVIT12A8C1451C1      1\n",
       "48373583  b7815dbb206eb2831ce0fe040d0aa537e2e800f7  SOUSMXX12AB0185C24      1\n",
       "48373584  b7815dbb206eb2831ce0fe040d0aa537e2e800f7  SOWYSKH12AF72A303A      3\n",
       "48373585  b7815dbb206eb2831ce0fe040d0aa537e2e800f7  SOYYFLV12A58A7A88F      1\n",
       "\n",
       "[48373586 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>song_id</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n      <td>SOAKIMP12A8C130995</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n      <td>SOAPDEY12A81C210A9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n      <td>SOBBMDR12A8C13253B</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n      <td>SOBFNSP12AF72A0E22</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n      <td>SOBFOVM12A58A7D494</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48373581</th>\n      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n      <td>SOUHHHH12AF729E4AF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>48373582</th>\n      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n      <td>SOUJVIT12A8C1451C1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>48373583</th>\n      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n      <td>SOUSMXX12AB0185C24</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>48373584</th>\n      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n      <td>SOWYSKH12AF72A303A</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>48373585</th>\n      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n      <td>SOYYFLV12A58A7A88F</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>48373586 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "source": [
    "filename = 'data/train_triplets.txt'\n",
    "id_df = pd.read_csv(filename, delimiter='\\t',names=['user_id','song_id','count'])\n",
    "id_df"
   ]
  },
  {
   "source": [
    "Read in dataframe linking song_id to song titles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-74-a6871d1b8ef7>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  tracks_df = pd.read_csv(track_file,delimiter='<SEP>',names=['track_id','song_id','artist','song'])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  track_id             song_id            artist  \\\n",
       "0       TRMMMYQ128F932D901  SOQMMHC12AB0180CB8  Faster Pussy cat   \n",
       "1       TRMMMKD128F425225D  SOVFVAK12A8C1350D9  Karkkiautomaatti   \n",
       "2       TRMMMRX128F93187D9  SOGTUKN12AB017F4F1    Hudson Mohawke   \n",
       "3       TRMMMCH128F425532C  SOBNYVR12A8C13558C       Yerba Brava   \n",
       "4       TRMMMWA128F426B589  SOHSBXH12A8C13B0DF        Der Mystic   \n",
       "...                    ...                 ...               ...   \n",
       "999995  TRYYYUS12903CD2DF0  SOTXAME12AB018F136      Kiko Navarro   \n",
       "999996  TRYYYJO128F426DA37  SOXQYIQ12A8C137FBB     Kuldeep Manak   \n",
       "999997  TRYYYMG128F4260ECA  SOHODZI12A8C137BB3    Gabriel Le Mar   \n",
       "999998  TRYYYDJ128F9310A21  SOLXGOR12A81C21EB7             Elude   \n",
       "999999  TRYYYVU12903CD01E3  SOWXJXQ12AB0189F43             Texta   \n",
       "\n",
       "                                        song  \n",
       "0                               Silent Night  \n",
       "1                                Tanssi vaan  \n",
       "2                          No One Could Ever  \n",
       "3                             Si Vos QuerÃ©s  \n",
       "4                           Tangle Of Aspens  \n",
       "...                                      ...  \n",
       "999995                       O Samba Da Vida  \n",
       "999996                          Jago Chhadeo  \n",
       "999997                               Novemba  \n",
       "999998                               Faraday  \n",
       "999999  Fernweh feat. Sektion KuchikÃ¤schtli  \n",
       "\n",
       "[1000000 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_id</th>\n      <th>song_id</th>\n      <th>artist</th>\n      <th>song</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRMMMYQ128F932D901</td>\n      <td>SOQMMHC12AB0180CB8</td>\n      <td>Faster Pussy cat</td>\n      <td>Silent Night</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRMMMKD128F425225D</td>\n      <td>SOVFVAK12A8C1350D9</td>\n      <td>Karkkiautomaatti</td>\n      <td>Tanssi vaan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRMMMRX128F93187D9</td>\n      <td>SOGTUKN12AB017F4F1</td>\n      <td>Hudson Mohawke</td>\n      <td>No One Could Ever</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRMMMCH128F425532C</td>\n      <td>SOBNYVR12A8C13558C</td>\n      <td>Yerba Brava</td>\n      <td>Si Vos QuerÃ©s</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRMMMWA128F426B589</td>\n      <td>SOHSBXH12A8C13B0DF</td>\n      <td>Der Mystic</td>\n      <td>Tangle Of Aspens</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>999995</th>\n      <td>TRYYYUS12903CD2DF0</td>\n      <td>SOTXAME12AB018F136</td>\n      <td>Kiko Navarro</td>\n      <td>O Samba Da Vida</td>\n    </tr>\n    <tr>\n      <th>999996</th>\n      <td>TRYYYJO128F426DA37</td>\n      <td>SOXQYIQ12A8C137FBB</td>\n      <td>Kuldeep Manak</td>\n      <td>Jago Chhadeo</td>\n    </tr>\n    <tr>\n      <th>999997</th>\n      <td>TRYYYMG128F4260ECA</td>\n      <td>SOHODZI12A8C137BB3</td>\n      <td>Gabriel Le Mar</td>\n      <td>Novemba</td>\n    </tr>\n    <tr>\n      <th>999998</th>\n      <td>TRYYYDJ128F9310A21</td>\n      <td>SOLXGOR12A81C21EB7</td>\n      <td>Elude</td>\n      <td>Faraday</td>\n    </tr>\n    <tr>\n      <th>999999</th>\n      <td>TRYYYVU12903CD01E3</td>\n      <td>SOWXJXQ12AB0189F43</td>\n      <td>Texta</td>\n      <td>Fernweh feat. Sektion KuchikÃ¤schtli</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000000 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "track_file = 'data/unique_tracks.txt'\n",
    "tracks_df = pd.read_csv(track_file,delimiter='<SEP>',names=['track_id','song_id','artist','song'])\n",
    "tracks_df"
   ]
  },
  {
   "source": [
    "Now join those two dataframes on the shared song id"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    track_id             song_id            artist  \\\n",
       "0         TRMMMYQ128F932D901  SOQMMHC12AB0180CB8  Faster Pussy cat   \n",
       "1         TRMMMYQ128F932D901  SOQMMHC12AB0180CB8  Faster Pussy cat   \n",
       "2         TRMMMYQ128F932D901  SOQMMHC12AB0180CB8  Faster Pussy cat   \n",
       "3         TRMMMCH128F425532C  SOBNYVR12A8C13558C       Yerba Brava   \n",
       "4         TRMMMCH128F425532C  SOBNYVR12A8C13558C       Yerba Brava   \n",
       "...                      ...                 ...               ...   \n",
       "49664523  TRYYYZM128F428E804  SOBODSE12A8C13EBD6           SKYCLAD   \n",
       "49664524  TRYYYZM128F428E804  SOBODSE12A8C13EBD6           SKYCLAD   \n",
       "49664525  TRYYYON128F932585A  SOWCNSN12AB018070F        Loose Shus   \n",
       "49664526  TRYYYON128F932585A  SOWCNSN12AB018070F        Loose Shus   \n",
       "49664527  TRYYYVU12903CD01E3  SOWXJXQ12AB0189F43             Texta   \n",
       "\n",
       "                                          song  \\\n",
       "0                                 Silent Night   \n",
       "1                                 Silent Night   \n",
       "2                                 Silent Night   \n",
       "3                               Si Vos QuerÃ©s   \n",
       "4                               Si Vos QuerÃ©s   \n",
       "...                                        ...   \n",
       "49664523                     Inequality Street   \n",
       "49664524                     Inequality Street   \n",
       "49664525              Taurus (Keenhouse Remix)   \n",
       "49664526              Taurus (Keenhouse Remix)   \n",
       "49664527  Fernweh feat. Sektion KuchikÃ¤schtli   \n",
       "\n",
       "                                           user_id  count  \n",
       "0         3a05343210b5e4b6308193bcd00242d326bd9b36      1  \n",
       "1         93f24a7eb6742300414e7b8d4fefddf3f90c3db7      6  \n",
       "2         53f8a04762e391eb0efb812b7352e4d598a48b2c      1  \n",
       "3         baf8f44f7f23ca9671be11ff296df32a09f4406d      1  \n",
       "4         c59ecc2ed2f13812c69a65c02d9847d255fa8ecf      1  \n",
       "...                                            ...    ...  \n",
       "49664523  9ab8ef65e878846e1e2b4e1109ffa56b1a2c09bb      1  \n",
       "49664524  56d29a12dbd87b619256e349d967fef6e1a46698      1  \n",
       "49664525  aeb227fce19da4e5a1d88f53df2ac0eb3ede4f74      1  \n",
       "49664526  95061cbe060d2500ce8af2bc71bcd9a6512352ba      2  \n",
       "49664527  73761dab0fa190edb18a2b21c4cfcf76d7cd8474      1  \n",
       "\n",
       "[49664528 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_id</th>\n      <th>song_id</th>\n      <th>artist</th>\n      <th>song</th>\n      <th>user_id</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRMMMYQ128F932D901</td>\n      <td>SOQMMHC12AB0180CB8</td>\n      <td>Faster Pussy cat</td>\n      <td>Silent Night</td>\n      <td>3a05343210b5e4b6308193bcd00242d326bd9b36</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRMMMYQ128F932D901</td>\n      <td>SOQMMHC12AB0180CB8</td>\n      <td>Faster Pussy cat</td>\n      <td>Silent Night</td>\n      <td>93f24a7eb6742300414e7b8d4fefddf3f90c3db7</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRMMMYQ128F932D901</td>\n      <td>SOQMMHC12AB0180CB8</td>\n      <td>Faster Pussy cat</td>\n      <td>Silent Night</td>\n      <td>53f8a04762e391eb0efb812b7352e4d598a48b2c</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRMMMCH128F425532C</td>\n      <td>SOBNYVR12A8C13558C</td>\n      <td>Yerba Brava</td>\n      <td>Si Vos QuerÃ©s</td>\n      <td>baf8f44f7f23ca9671be11ff296df32a09f4406d</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRMMMCH128F425532C</td>\n      <td>SOBNYVR12A8C13558C</td>\n      <td>Yerba Brava</td>\n      <td>Si Vos QuerÃ©s</td>\n      <td>c59ecc2ed2f13812c69a65c02d9847d255fa8ecf</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49664523</th>\n      <td>TRYYYZM128F428E804</td>\n      <td>SOBODSE12A8C13EBD6</td>\n      <td>SKYCLAD</td>\n      <td>Inequality Street</td>\n      <td>9ab8ef65e878846e1e2b4e1109ffa56b1a2c09bb</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49664524</th>\n      <td>TRYYYZM128F428E804</td>\n      <td>SOBODSE12A8C13EBD6</td>\n      <td>SKYCLAD</td>\n      <td>Inequality Street</td>\n      <td>56d29a12dbd87b619256e349d967fef6e1a46698</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49664525</th>\n      <td>TRYYYON128F932585A</td>\n      <td>SOWCNSN12AB018070F</td>\n      <td>Loose Shus</td>\n      <td>Taurus (Keenhouse Remix)</td>\n      <td>aeb227fce19da4e5a1d88f53df2ac0eb3ede4f74</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49664526</th>\n      <td>TRYYYON128F932585A</td>\n      <td>SOWCNSN12AB018070F</td>\n      <td>Loose Shus</td>\n      <td>Taurus (Keenhouse Remix)</td>\n      <td>95061cbe060d2500ce8af2bc71bcd9a6512352ba</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>49664527</th>\n      <td>TRYYYVU12903CD01E3</td>\n      <td>SOWXJXQ12AB0189F43</td>\n      <td>Texta</td>\n      <td>Fernweh feat. Sektion KuchikÃ¤schtli</td>\n      <td>73761dab0fa190edb18a2b21c4cfcf76d7cd8474</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>49664528 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 75
    }
   ],
   "source": [
    "df = pd.merge(tracks_df, id_df, how='inner', on='song_id')\n",
    "df"
   ]
  },
  {
   "source": [
    "Now we read in our lyric data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found pickle file for current config, using that\n"
     ]
    }
   ],
   "source": [
    "sample_frac = 1.0\n",
    "\n",
    "pickle_filename = f\"data/lyrec_df_{sample_frac}.pkl\"\n",
    "print(\"Found pickle file for current config, using that\")\n",
    "giant_df = pd.read_pickle(pickle_filename)\n",
    "\n",
    "# giant_df = giant_df.sample(frac=1, random_state=1)\n",
    "lyric_df = giant_df.sample(frac=0.02,random_state = 1)\n",
    "#lyric_file = 'data/first1000.csv'\n",
    "\n",
    "#lyric_df = pd.read_csv(lyric_file)\n",
    "lyric_df['original_index'] = lyric_df.index\n",
    "# this is needed incase we want to map back to the original index in the lyric table after all the joins\n",
    "# lyric_df"
   ]
  },
  {
   "source": [
    "Now we join our data all together by matching on both artist and song name - this helps filter out duplicate song names, and there were many!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  track_id             song_id               artist  \\\n",
       "0       TRMRUDG12903CBD7B8  SOPJJHT12A8C13F776  Great Lake Swimmers   \n",
       "1       TRMRUDG12903CBD7B8  SOPJJHT12A8C13F776  Great Lake Swimmers   \n",
       "2       TRMRUDG12903CBD7B8  SOPJJHT12A8C13F776  Great Lake Swimmers   \n",
       "3       TRMRUDG12903CBD7B8  SOPJJHT12A8C13F776  Great Lake Swimmers   \n",
       "4       TRMRUDG12903CBD7B8  SOPJJHT12A8C13F776  Great Lake Swimmers   \n",
       "...                    ...                 ...                  ...   \n",
       "267529  TRYXYZE128F1453C10  SOXCHOX12A6D4F72C7                  Tru   \n",
       "267530  TRYXYZE128F1453C10  SOXCHOX12A6D4F72C7                  Tru   \n",
       "267531  TRYXYZE128F1453C10  SOXCHOX12A6D4F72C7                  Tru   \n",
       "267532  TRYXYZE128F1453C10  SOXCHOX12A6D4F72C7                  Tru   \n",
       "267533  TRYXYZE128F1453C10  SOXCHOX12A6D4F72C7                  Tru   \n",
       "\n",
       "                    song                                   user_id  count  \\\n",
       "0       Bodies and Minds  4e11f45d732f4861772b2906f81a7d384552ad12      1   \n",
       "1       Bodies and Minds  ec6c0cc39f69f203a7b2ad930192e3443381ad8a      1   \n",
       "2       Bodies and Minds  20918542b3d803e1f063514186615bc4617414d4      1   \n",
       "3       Bodies and Minds  017db04a4c7362734eb16838980b3a1a3bdb2754      9   \n",
       "4       Bodies and Minds  60fbfa788e5efbe85afb07e97e5c6d60820e0d53      2   \n",
       "...                  ...                                       ...    ...   \n",
       "267529        Freak Hoes  7963b31782812df9c11a9ff1f1a63d5c0e1520f4      1   \n",
       "267530        Freak Hoes  816e685c0f17f015168ed1141a43c07261472923      3   \n",
       "267531        Freak Hoes  a524e3a9e9086686ad3dd1d04b81f9d9588fc10e      1   \n",
       "267532        Freak Hoes  274084b3cb0b9f2e9f5374dc5d4ad2838f498acb      1   \n",
       "267533        Freak Hoes  181c68cdd57333724dd2be7af2607d9eae959f7b      1   \n",
       "\n",
       "                       Band  \\\n",
       "0       Great Lake Swimmers   \n",
       "1       Great Lake Swimmers   \n",
       "2       Great Lake Swimmers   \n",
       "3       Great Lake Swimmers   \n",
       "4       Great Lake Swimmers   \n",
       "...                     ...   \n",
       "267529                  Tru   \n",
       "267530                  Tru   \n",
       "267531                  Tru   \n",
       "267532                  Tru   \n",
       "267533                  Tru   \n",
       "\n",
       "                                                   Lyrics              Song  \\\n",
       "0       O hold me O holy O golden O grace\\nDown here i...  Bodies and Minds   \n",
       "1       O hold me O holy O golden O grace\\nDown here i...  Bodies and Minds   \n",
       "2       O hold me O holy O golden O grace\\nDown here i...  Bodies and Minds   \n",
       "3       O hold me O holy O golden O grace\\nDown here i...  Bodies and Minds   \n",
       "4       O hold me O holy O golden O grace\\nDown here i...  Bodies and Minds   \n",
       "...                                                   ...               ...   \n",
       "267529  Hey check this out miss thang or should I say ...        Freak Hoes   \n",
       "267530  Hey check this out miss thang or should I say ...        Freak Hoes   \n",
       "267531  Hey check this out miss thang or should I say ...        Freak Hoes   \n",
       "267532  Hey check this out miss thang or should I say ...        Freak Hoes   \n",
       "267533  Hey check this out miss thang or should I say ...        Freak Hoes   \n",
       "\n",
       "        original_index  \n",
       "0               279991  \n",
       "1               279991  \n",
       "2               279991  \n",
       "3               279991  \n",
       "4               279991  \n",
       "...                ...  \n",
       "267529          168422  \n",
       "267530          168422  \n",
       "267531          168422  \n",
       "267532          168422  \n",
       "267533          168422  \n",
       "\n",
       "[267534 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_id</th>\n      <th>song_id</th>\n      <th>artist</th>\n      <th>song</th>\n      <th>user_id</th>\n      <th>count</th>\n      <th>Band</th>\n      <th>Lyrics</th>\n      <th>Song</th>\n      <th>original_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRMRUDG12903CBD7B8</td>\n      <td>SOPJJHT12A8C13F776</td>\n      <td>Great Lake Swimmers</td>\n      <td>Bodies and Minds</td>\n      <td>4e11f45d732f4861772b2906f81a7d384552ad12</td>\n      <td>1</td>\n      <td>Great Lake Swimmers</td>\n      <td>O hold me O holy O golden O grace\\nDown here i...</td>\n      <td>Bodies and Minds</td>\n      <td>279991</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRMRUDG12903CBD7B8</td>\n      <td>SOPJJHT12A8C13F776</td>\n      <td>Great Lake Swimmers</td>\n      <td>Bodies and Minds</td>\n      <td>ec6c0cc39f69f203a7b2ad930192e3443381ad8a</td>\n      <td>1</td>\n      <td>Great Lake Swimmers</td>\n      <td>O hold me O holy O golden O grace\\nDown here i...</td>\n      <td>Bodies and Minds</td>\n      <td>279991</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRMRUDG12903CBD7B8</td>\n      <td>SOPJJHT12A8C13F776</td>\n      <td>Great Lake Swimmers</td>\n      <td>Bodies and Minds</td>\n      <td>20918542b3d803e1f063514186615bc4617414d4</td>\n      <td>1</td>\n      <td>Great Lake Swimmers</td>\n      <td>O hold me O holy O golden O grace\\nDown here i...</td>\n      <td>Bodies and Minds</td>\n      <td>279991</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRMRUDG12903CBD7B8</td>\n      <td>SOPJJHT12A8C13F776</td>\n      <td>Great Lake Swimmers</td>\n      <td>Bodies and Minds</td>\n      <td>017db04a4c7362734eb16838980b3a1a3bdb2754</td>\n      <td>9</td>\n      <td>Great Lake Swimmers</td>\n      <td>O hold me O holy O golden O grace\\nDown here i...</td>\n      <td>Bodies and Minds</td>\n      <td>279991</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRMRUDG12903CBD7B8</td>\n      <td>SOPJJHT12A8C13F776</td>\n      <td>Great Lake Swimmers</td>\n      <td>Bodies and Minds</td>\n      <td>60fbfa788e5efbe85afb07e97e5c6d60820e0d53</td>\n      <td>2</td>\n      <td>Great Lake Swimmers</td>\n      <td>O hold me O holy O golden O grace\\nDown here i...</td>\n      <td>Bodies and Minds</td>\n      <td>279991</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>267529</th>\n      <td>TRYXYZE128F1453C10</td>\n      <td>SOXCHOX12A6D4F72C7</td>\n      <td>Tru</td>\n      <td>Freak Hoes</td>\n      <td>7963b31782812df9c11a9ff1f1a63d5c0e1520f4</td>\n      <td>1</td>\n      <td>Tru</td>\n      <td>Hey check this out miss thang or should I say ...</td>\n      <td>Freak Hoes</td>\n      <td>168422</td>\n    </tr>\n    <tr>\n      <th>267530</th>\n      <td>TRYXYZE128F1453C10</td>\n      <td>SOXCHOX12A6D4F72C7</td>\n      <td>Tru</td>\n      <td>Freak Hoes</td>\n      <td>816e685c0f17f015168ed1141a43c07261472923</td>\n      <td>3</td>\n      <td>Tru</td>\n      <td>Hey check this out miss thang or should I say ...</td>\n      <td>Freak Hoes</td>\n      <td>168422</td>\n    </tr>\n    <tr>\n      <th>267531</th>\n      <td>TRYXYZE128F1453C10</td>\n      <td>SOXCHOX12A6D4F72C7</td>\n      <td>Tru</td>\n      <td>Freak Hoes</td>\n      <td>a524e3a9e9086686ad3dd1d04b81f9d9588fc10e</td>\n      <td>1</td>\n      <td>Tru</td>\n      <td>Hey check this out miss thang or should I say ...</td>\n      <td>Freak Hoes</td>\n      <td>168422</td>\n    </tr>\n    <tr>\n      <th>267532</th>\n      <td>TRYXYZE128F1453C10</td>\n      <td>SOXCHOX12A6D4F72C7</td>\n      <td>Tru</td>\n      <td>Freak Hoes</td>\n      <td>274084b3cb0b9f2e9f5374dc5d4ad2838f498acb</td>\n      <td>1</td>\n      <td>Tru</td>\n      <td>Hey check this out miss thang or should I say ...</td>\n      <td>Freak Hoes</td>\n      <td>168422</td>\n    </tr>\n    <tr>\n      <th>267533</th>\n      <td>TRYXYZE128F1453C10</td>\n      <td>SOXCHOX12A6D4F72C7</td>\n      <td>Tru</td>\n      <td>Freak Hoes</td>\n      <td>181c68cdd57333724dd2be7af2607d9eae959f7b</td>\n      <td>1</td>\n      <td>Tru</td>\n      <td>Hey check this out miss thang or should I say ...</td>\n      <td>Freak Hoes</td>\n      <td>168422</td>\n    </tr>\n  </tbody>\n</table>\n<p>267534 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "df = pd.merge(df, lyric_df, how='inner',left_on=['song','artist'],right_on=['Song','Band'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unique users:  196191\nunique songs:  779\n"
     ]
    }
   ],
   "source": [
    "# drop columns we won't need\n",
    "df.drop(columns=['track_id'])\n",
    "\n",
    "print(\"unique users: \", df['user_id'].unique().size)\n",
    "print(\"unique songs: \", df['song_id'].unique().size)"
   ]
  },
  {
   "source": [
    "This step allows us to easily re-index our song id's into an easy to use form. We map song_id -> integer value that can be used to index our matrices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_SongID = df['song_id'].unique()\n",
    "unique_UserID = df['user_id'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    j += 1\n",
    "j = 0\n",
    "song_old2new_id_dict = dict() \n",
    "for i in unique_SongID:\n",
    "    song_old2new_id_dict[i] = j\n",
    "    j += 1\n",
    "    \n",
    "# Then, use the generated dictionaries to reindex UserID and Song in the df\n",
    "for j in range(len(df)):\n",
    "    df.at[j, 'user_id'] = user_old2new_id_dict[df.at[j, 'user_id']]\n",
    "    df.at[j, 'song_id'] = song_old2new_id_dict[df.at[j, 'song_id']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  track_id song_id               artist              song  \\\n",
       "0       TRMRUDG12903CBD7B8       0  Great Lake Swimmers  Bodies and Minds   \n",
       "1       TRMRUDG12903CBD7B8       0  Great Lake Swimmers  Bodies and Minds   \n",
       "2       TRMRUDG12903CBD7B8       0  Great Lake Swimmers  Bodies and Minds   \n",
       "3       TRMRUDG12903CBD7B8       0  Great Lake Swimmers  Bodies and Minds   \n",
       "4       TRMRUDG12903CBD7B8       0  Great Lake Swimmers  Bodies and Minds   \n",
       "...                    ...     ...                  ...               ...   \n",
       "267529  TRYXYZE128F1453C10     778                  Tru        Freak Hoes   \n",
       "267530  TRYXYZE128F1453C10     778                  Tru        Freak Hoes   \n",
       "267531  TRYXYZE128F1453C10     778                  Tru        Freak Hoes   \n",
       "267532  TRYXYZE128F1453C10     778                  Tru        Freak Hoes   \n",
       "267533  TRYXYZE128F1453C10     778                  Tru        Freak Hoes   \n",
       "\n",
       "       user_id  count                 Band  \\\n",
       "0            0      1  Great Lake Swimmers   \n",
       "1            1      1  Great Lake Swimmers   \n",
       "2            2      1  Great Lake Swimmers   \n",
       "3            3      9  Great Lake Swimmers   \n",
       "4            4      2  Great Lake Swimmers   \n",
       "...        ...    ...                  ...   \n",
       "267529  196188      1                  Tru   \n",
       "267530   58138      3                  Tru   \n",
       "267531  196189      1                  Tru   \n",
       "267532   42576      1                  Tru   \n",
       "267533  196190      1                  Tru   \n",
       "\n",
       "                                                   Lyrics              Song  \\\n",
       "0       O hold me O holy O golden O grace\\nDown here i...  Bodies and Minds   \n",
       "1       O hold me O holy O golden O grace\\nDown here i...  Bodies and Minds   \n",
       "2       O hold me O holy O golden O grace\\nDown here i...  Bodies and Minds   \n",
       "3       O hold me O holy O golden O grace\\nDown here i...  Bodies and Minds   \n",
       "4       O hold me O holy O golden O grace\\nDown here i...  Bodies and Minds   \n",
       "...                                                   ...               ...   \n",
       "267529  Hey check this out miss thang or should I say ...        Freak Hoes   \n",
       "267530  Hey check this out miss thang or should I say ...        Freak Hoes   \n",
       "267531  Hey check this out miss thang or should I say ...        Freak Hoes   \n",
       "267532  Hey check this out miss thang or should I say ...        Freak Hoes   \n",
       "267533  Hey check this out miss thang or should I say ...        Freak Hoes   \n",
       "\n",
       "        original_index  \n",
       "0               279991  \n",
       "1               279991  \n",
       "2               279991  \n",
       "3               279991  \n",
       "4               279991  \n",
       "...                ...  \n",
       "267529          168422  \n",
       "267530          168422  \n",
       "267531          168422  \n",
       "267532          168422  \n",
       "267533          168422  \n",
       "\n",
       "[267534 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_id</th>\n      <th>song_id</th>\n      <th>artist</th>\n      <th>song</th>\n      <th>user_id</th>\n      <th>count</th>\n      <th>Band</th>\n      <th>Lyrics</th>\n      <th>Song</th>\n      <th>original_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRMRUDG12903CBD7B8</td>\n      <td>0</td>\n      <td>Great Lake Swimmers</td>\n      <td>Bodies and Minds</td>\n      <td>0</td>\n      <td>1</td>\n      <td>Great Lake Swimmers</td>\n      <td>O hold me O holy O golden O grace\\nDown here i...</td>\n      <td>Bodies and Minds</td>\n      <td>279991</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRMRUDG12903CBD7B8</td>\n      <td>0</td>\n      <td>Great Lake Swimmers</td>\n      <td>Bodies and Minds</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Great Lake Swimmers</td>\n      <td>O hold me O holy O golden O grace\\nDown here i...</td>\n      <td>Bodies and Minds</td>\n      <td>279991</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRMRUDG12903CBD7B8</td>\n      <td>0</td>\n      <td>Great Lake Swimmers</td>\n      <td>Bodies and Minds</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Great Lake Swimmers</td>\n      <td>O hold me O holy O golden O grace\\nDown here i...</td>\n      <td>Bodies and Minds</td>\n      <td>279991</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRMRUDG12903CBD7B8</td>\n      <td>0</td>\n      <td>Great Lake Swimmers</td>\n      <td>Bodies and Minds</td>\n      <td>3</td>\n      <td>9</td>\n      <td>Great Lake Swimmers</td>\n      <td>O hold me O holy O golden O grace\\nDown here i...</td>\n      <td>Bodies and Minds</td>\n      <td>279991</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRMRUDG12903CBD7B8</td>\n      <td>0</td>\n      <td>Great Lake Swimmers</td>\n      <td>Bodies and Minds</td>\n      <td>4</td>\n      <td>2</td>\n      <td>Great Lake Swimmers</td>\n      <td>O hold me O holy O golden O grace\\nDown here i...</td>\n      <td>Bodies and Minds</td>\n      <td>279991</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>267529</th>\n      <td>TRYXYZE128F1453C10</td>\n      <td>778</td>\n      <td>Tru</td>\n      <td>Freak Hoes</td>\n      <td>196188</td>\n      <td>1</td>\n      <td>Tru</td>\n      <td>Hey check this out miss thang or should I say ...</td>\n      <td>Freak Hoes</td>\n      <td>168422</td>\n    </tr>\n    <tr>\n      <th>267530</th>\n      <td>TRYXYZE128F1453C10</td>\n      <td>778</td>\n      <td>Tru</td>\n      <td>Freak Hoes</td>\n      <td>58138</td>\n      <td>3</td>\n      <td>Tru</td>\n      <td>Hey check this out miss thang or should I say ...</td>\n      <td>Freak Hoes</td>\n      <td>168422</td>\n    </tr>\n    <tr>\n      <th>267531</th>\n      <td>TRYXYZE128F1453C10</td>\n      <td>778</td>\n      <td>Tru</td>\n      <td>Freak Hoes</td>\n      <td>196189</td>\n      <td>1</td>\n      <td>Tru</td>\n      <td>Hey check this out miss thang or should I say ...</td>\n      <td>Freak Hoes</td>\n      <td>168422</td>\n    </tr>\n    <tr>\n      <th>267532</th>\n      <td>TRYXYZE128F1453C10</td>\n      <td>778</td>\n      <td>Tru</td>\n      <td>Freak Hoes</td>\n      <td>42576</td>\n      <td>1</td>\n      <td>Tru</td>\n      <td>Hey check this out miss thang or should I say ...</td>\n      <td>Freak Hoes</td>\n      <td>168422</td>\n    </tr>\n    <tr>\n      <th>267533</th>\n      <td>TRYXYZE128F1453C10</td>\n      <td>778</td>\n      <td>Tru</td>\n      <td>Freak Hoes</td>\n      <td>196190</td>\n      <td>1</td>\n      <td>Tru</td>\n      <td>Hey check this out miss thang or should I say ...</td>\n      <td>Freak Hoes</td>\n      <td>168422</td>\n    </tr>\n  </tbody>\n</table>\n<p>267534 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "source": [
    "# Part 2: Create Train & Test data\n",
    "\n",
    "Originally, we wanted the matrices we work with to be of dimension (users, songs), where each entry (u,s) is the number of times the user has listened to that song. However, we realized it is better to fill the matrices with the *fraction* of total listens for that particular user. This way, the model is agnostic of different users listening at differing rates.  \n",
    "\n",
    "First we sample our df to create our train and test data, and check for overlap\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1872740\n802600\nno overlap!\n"
     ]
    }
   ],
   "source": [
    "train_df = df.sample(frac=0.7, random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "print(train_df.size)\n",
    "print(test_df.size)\n",
    "\n",
    "if(train_df.size + test_df.size == df.size):\n",
    "    print(\"no overlap!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(196191, 779)\n(196191, 779)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "num_user = len(df['user_id'].unique())\n",
    "num_song = len(df['song_id'].unique())\n",
    "\n",
    "train_mat = coo_matrix((train_df['count'].values, (train_df['user_id'].values, train_df['song_id'].values)), shape=(num_user, num_song)).toarray().astype(float)\n",
    "test_mat = coo_matrix((test_df['count'].values, (test_df['user_id'].values, test_df['song_id'].values)), shape=(num_user, num_song)).toarray().astype(float)\n",
    "\n",
    "# fill nan values\n",
    "train_mat[np.isnan(train_mat)] = 0\n",
    "test_mat[np.isnan(test_mat)] = 0\n",
    "\n",
    "print(train_mat.shape)\n",
    "print(test_mat.shape)"
   ]
  },
  {
   "source": [
    "Standardize the user counts, so every song listen value becomes a percentage of user listens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat_counts = np.sum(train_mat,axis=1).T\n",
    "test_mat_counts = np.sum(test_mat,axis=1).T\n",
    "\n",
    "for user_id in range(num_user):\n",
    "    \n",
    "    denom = 1\n",
    "    if train_mat_counts[user_id] != 0:\n",
    "        denom = train_mat_counts[user_id]\n",
    "\n",
    "    train_mat[user_id] = train_mat[user_id]/denom\n",
    "    test_mat[user_id] = test_mat[user_id]/denom"
   ]
  },
  {
   "source": [
    "# Part 3: Model Creation\n",
    "\n",
    "We will create and validate 3 models - Bag of Words, Bert, and a user-user MF model. All of these models will essentially generate latent features in some way given our (user, song) train data, and then generate predictions for fraction of listens. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3a: Bag of Words\n",
    "\n",
    "Our simple bag of words model has a few steps:\n",
    "\n",
    "1. For all lyrics, count the top N most frequent, nontrivial terms\n",
    "2. For each song, generate an N-length vector containing counts of those terms\n",
    "3. For each user, generate a user profile songs weighted by listens\n",
    "4. Produce a prediction for each user-song pair with the product of the user profile and song "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "\n",
    "BAG_SIZE = 1000\n",
    "def get_top_values(d,N = BAG_SIZE):\n",
    "    return list(sorted(d.items(), key = itemgetter(1), reverse = True)[:N])\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# creating  \"master\" dictionary\n",
    "word2count = {}\n",
    "\n",
    "for song_id in range(num_song):\n",
    "\n",
    "    # this will get the first index where song_id appears\n",
    "    lyric_idx = df[df['song_id']==song_id].index.values[0]\n",
    "    \n",
    "    # get song lyrics at that index\n",
    "    song_lyric = df.Lyrics.iloc[lyric_idx]\n",
    "\n",
    "    # clean song lyrics\n",
    "    song_lyric = song_lyric.lower()\n",
    "    song_lyric = re.sub(r'\\W', ' ', song_lyric)\n",
    "    song_lyic = re.sub(r'\\s+', ' ', song_lyric)\n",
    "    song_lyric = re.sub(r'\\r|\\n', ' ', song_lyric)\n",
    "\n",
    "    # get word counts for songs omitting stopwords\n",
    "    words = nltk.word_tokenize(song_lyric)\n",
    "    for word in words:\n",
    "\n",
    "        if word not in stop_words:\n",
    "\n",
    "            if word not in word2count:\n",
    "                word2count[word] = 1\n",
    "            else:\n",
    "                word2count[word] += 1\n",
    "\n",
    "# get the top ranked words for our songs\n",
    "\n",
    "word2count_list = get_top_values(word2count)\n",
    "word2count = dict(word2count_list)\n",
    "word_ranks = {w2c[0]: r for r, w2c in enumerate(word2count_list)}\n",
    "\n",
    "# Now pass through again and create vectors\n",
    "\n",
    "song_counts = np.zeros([num_song, BAG_SIZE])\n",
    "\n",
    "for song_id in range(num_song):\n",
    "\n",
    "    lyric_idx = df[df['song_id']==song_id].index.values[0]\n",
    "    \n",
    "    # get song lyrics at that index\n",
    "    song_lyric = df.Lyrics.iloc[lyric_idx]\n",
    "\n",
    "    # clean song lyrics\n",
    "    song_lyric = song_lyric.lower()\n",
    "    song_lyric = re.sub(r'\\W', ' ', song_lyric)\n",
    "    song_lyic = re.sub(r'\\s+', ' ', song_lyric)\n",
    "    song_lyric = re.sub(r'\\r|\\n', ' ', song_lyric)\n",
    "\n",
    "    words = nltk.word_tokenize(song_lyric)\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_ranks:\n",
    "            word_idx = word_ranks[word]\n",
    "            song_counts[song_id][word_idx] += 1\n",
    "\n",
    "song_pairwise_similarities = pairwise.cosine_similarity(song_counts)"
   ]
  },
  {
   "source": [
    "\n",
    "Generate a user profile based on the tastes of each user. This will be a weighted average  \n",
    "of all the songs the user has listened to, based on relative number of listens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_user_profiles =  np.zeros([num_user, BAG_SIZE])\n",
    "bow_user_profiles = train_mat @ song_counts\n"
   ]
  },
  {
   "source": [
    "Create predictions for bag of words by getting the product of user profiles with songs. Ignore users with no listens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.03755018, 0.00034544, 0.00025208, ..., 0.00069088, 0.00079358,\n",
       "        0.00125105],\n",
       "       [0.05656789, 0.        , 0.00013025, ..., 0.0001563 , 0.00071638,\n",
       "        0.00114621],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.00079016, 0.00013944, ..., 0.0008134 , 0.000581  ,\n",
       "        0.02337958],\n",
       "       [0.        , 0.00079016, 0.00013944, ..., 0.0008134 , 0.000581  ,\n",
       "        0.02337958]])"
      ]
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "bow_prediction_mat = bow_user_profiles @ song_counts.T\n",
    "\n",
    "bow_prediction_mat_counts = np.sum(bow_prediction_mat,axis=1).T\n",
    "\n",
    "for user_id in range(num_user):\n",
    "    \n",
    "    denom = 1\n",
    "    if bow_prediction_mat_counts[user_id] != 0:\n",
    "        denom = bow_prediction_mat_counts[user_id]\n",
    "\n",
    "    bow_prediction_mat[user_id] = bow_prediction_mat[user_id]/denom\n",
    "\n",
    "bow_prediction_mat"
   ]
  },
  {
   "source": [
    "##3b: BERT\n",
    "\n",
    "Our BERT model is similarly structured, except how we get our latent features\n",
    "\n",
    "1. Compute latent features for lyrics by feeding into BERT, a pretrained model \n",
    "2. For each user, generate a user profile that is weighted by listens. So each user profile should be as long as the latent dimension\n",
    "3. Produce a prediction for each user-song pair with the product of the user profile and latent representation of the song\n",
    "\n",
    "First however, we have to read in our document embeddings that we computed earlier and wrote to a file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frac = 1.0\n",
    "filepath = f\"data/lyrec_embeddings_{sample_frac}.pkl.npy\"\n",
    "\n",
    "if os.path.isfile(filepath):\n",
    "    got_from_file = True\n",
    "    document_embeddings = np.load(filepath)\n",
    "else:\n",
    "    print('no file path')\n",
    "\n",
    "if not got_from_file:\n",
    "    np.save(filepath, document_embeddings)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(516174, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "document_embeddings.shape"
   ]
  },
  {
   "source": [
    "Now we use the original index to map back to the correct lyrics, since BERT computed many redundant lyric representations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(516174, 768)\n",
      "(267534, 768)\n",
      "763\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# subset_embeddings = document_embeddings[df.original_index]\n",
    "\n",
    "# print(document_embeddings.shape)\n",
    "# print(subset_embeddings.shape)\n",
    "# print(len(np.unique(subset_embeddings, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(779, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "source": [
    "num_bert = document_embeddings.shape[1]\n",
    "bert_factors = np.zeros([num_song, num_bert])\n",
    "\n",
    "for song_id in range(num_song):\n",
    "\n",
    "    # get the original index where the first song id appears in the df\n",
    "    idx = df[df['song_id']==song_id].index.values[0]\n",
    "\n",
    "    original_idx = df.original_index.iloc[idx]\n",
    "\n",
    "    # get song lyrics at that index\n",
    "    bert_factors[song_id] = document_embeddings[original_idx]\n",
    "\n",
    "bert_factors.shape"
   ]
  },
  {
   "source": [
    "Compute bert user profiles as the weighted average of latent representations of songs, weighted by the listens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(196191, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "bert_user_profiles =  np.zeros([num_user, num_bert])\n",
    "\n",
    "bert_user_profiles = train_mat @ bert_factors\n",
    "\n",
    "bert_user_profiles.shape"
   ]
  },
  {
   "source": [
    "Get product of user profiles and song representations for predicted values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_prediction_mat = bert_user_profiles @ bert_factors.T\n",
    "bert_prediction_mat_counts = np.sum(bert_prediction_mat,axis=1).T\n",
    "\n",
    "for user_id in range(num_user):\n",
    "    \n",
    "    denom = 1\n",
    "    if bert_prediction_mat_counts[user_id] != 0:\n",
    "        denom = bert_prediction_mat_counts[user_id]\n",
    "\n",
    "    bert_prediction_mat[user_id] = bert_prediction_mat[user_id]/denom"
   ]
  },
  {
   "source": [
    "# Part 3: User-User MF\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_rmse(test_mat,pred_mat):\n",
    "\n",
    "    total = 0\n",
    "    n = pred_mat.size\n",
    "    unrated = 0\n",
    "\n",
    "    for user_id in range(num_user):\n",
    "\n",
    "        if np.sum(pred_mat[user_id])==0:\n",
    "            unrated += num_song\n",
    "        else:\n",
    "            total += np.sum(np.square(test_mat[user_id] - pred_mat[user_id]))\n",
    "    \n",
    "    return np.sqrt(total/(n-unrated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-3385d47f879b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnmf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNMF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mfactorization_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnmf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponents_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmf_mat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfactorization_matrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, W, H)\u001b[0m\n\u001b[0;32m   1281\u001b[0m                                 dtype=[np.float64, np.float32])\n\u001b[0;32m   1282\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1283\u001b[1;33m         W, H, n_iter_ = non_negative_factorization(\n\u001b[0m\u001b[0;32m   1284\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m             \u001b[0mupdate_H\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py\u001b[0m in \u001b[0;36mnon_negative_factorization\u001b[1;34m(X, W, H, n_components, init, update_H, solver, beta_loss, tol, max_iter, alpha, l1_ratio, regularization, random_state, verbose, shuffle)\u001b[0m\n\u001b[0;32m   1057\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cd'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1059\u001b[1;33m         W, H, n_iter = _fit_coordinate_descent(X, W, H, tol, max_iter,\n\u001b[0m\u001b[0;32m   1060\u001b[0m                                                \u001b[0ml1_reg_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_reg_H\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m                                                \u001b[0ml2_reg_W\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml2_reg_H\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py\u001b[0m in \u001b[0;36m_fit_coordinate_descent\u001b[1;34m(X, W, H, tol, max_iter, l1_reg_W, l1_reg_H, l2_reg_W, l2_reg_H, update_H, verbose, shuffle, random_state)\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[1;31m# Update H\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mupdate_H\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m             violation += _update_coordinate_descent(X.T, Ht, W, l1_reg_H,\n\u001b[0m\u001b[0;32m    514\u001b[0m                                                     l2_reg_H, shuffle, rng)\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py\u001b[0m in \u001b[0;36m_update_coordinate_descent\u001b[1;34m(X, W, Ht, l1_reg, l2_reg, shuffle, random_state)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[0mn_components\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m     \u001b[0mHHt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m     \u001b[0mXHt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iterations = [50,100,150,200,250,300]\n",
    "iter_rmse = []\n",
    "for i in iterations:\n",
    "    nmf = NMF(max_iter=i)\n",
    "    W = nmf.fit_transform(train_mat)\n",
    "    factorization_matrix = nmf.components_\n",
    "    mf_mat = np.dot(W,factorization_matrix)\n",
    "\n",
    "    iter_rmse.append(new_rmse(test_mat,mf_mat))\n",
    "\n",
    "plt.plot(iterations, iter_rmse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0.1 * i for i in range(10)]\n",
    "alpha_rmse = []\n",
    "for i in a:\n",
    "    nmf = NMF(alpha=i)\n",
    "    W = nmf.fit_transform(train_mat)\n",
    "    factorization_matrix = nmf.components_\n",
    "    mf_mat = np.dot(W,factorization_matrix)\n",
    "\n",
    "    alpha_rmse.append(new_rmse(test_mat,mf_mat))\n",
    "\n",
    "plt.plot(a, alpha_rmse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_ratio = [0.1 * i for i in range(10)]\n",
    "l1_rmse = []\n",
    "for i in a:\n",
    "    nmf = NMF(l1_ratio=i)\n",
    "    W = nmf.fit_transform(train_mat)\n",
    "    factorization_matrix = nmf.components_\n",
    "    mf_mat = np.dot(W,factorization_matrix)\n",
    "\n",
    "    l1_rmse.append(new_rmse(test_mat,mf_mat))\n",
    "\n",
    "plt.plot(l1_ratio, l1_rmse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal model\n",
    "nmf = NMF(iterations=,l1_ratio=,alpha=)\n",
    "W = nmf.fit_transform(train_mat)\n",
    "factorization_matrix = nmf.components_\n",
    "mf_mat = np.dot(W,factorization_matrix)"
   ]
  },
  {
   "source": [
    "Define a new rmse function that ignores users with zero listens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "baseline rmse:  0.7135563583690526\n",
      "user-user rmse:  0.18755203340460716\n",
      "bag of words rmse:  0.12378539712372252\n",
      "BERT rmse:  0.10575903232384942\n"
     ]
    }
   ],
   "source": [
    "print('baseline rmse: ', new_rmse(test_mat,np.random.rand(num_user,num_song)))\n",
    "print('user-user rmse: ', new_rmse(test_mat,mf_mat))\n",
    "print('bag of words rmse: ', new_rmse(test_mat,bow_prediction_mat))\n",
    "print('BERT rmse: ', new_rmse(test_mat,bert_prediction_mat))"
   ]
  }
 ]
}