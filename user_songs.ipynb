{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd04bba1d32cad56bd564e8db7f54bce5a3582261770c0fd88a009216e68a1efe96",
   "display_name": "Python 3.8.5 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "# Part 1: Joining our data\n",
    "\n",
    "Our data was obtained through 3 seperate csv files, so we need to join them all together   \n",
    "We start by reading the in dataframe containing user_id, song_id, and listen counts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                           user_id             song_id  count\n",
       "0         b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAKIMP12A8C130995      1\n",
       "1         b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOAPDEY12A81C210A9      1\n",
       "2         b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBBMDR12A8C13253B      2\n",
       "3         b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFNSP12AF72A0E22      1\n",
       "4         b80344d063b5ccb3212f76538f3d9e43d87dca9e  SOBFOVM12A58A7D494      1\n",
       "...                                            ...                 ...    ...\n",
       "48373581  b7815dbb206eb2831ce0fe040d0aa537e2e800f7  SOUHHHH12AF729E4AF      2\n",
       "48373582  b7815dbb206eb2831ce0fe040d0aa537e2e800f7  SOUJVIT12A8C1451C1      1\n",
       "48373583  b7815dbb206eb2831ce0fe040d0aa537e2e800f7  SOUSMXX12AB0185C24      1\n",
       "48373584  b7815dbb206eb2831ce0fe040d0aa537e2e800f7  SOWYSKH12AF72A303A      3\n",
       "48373585  b7815dbb206eb2831ce0fe040d0aa537e2e800f7  SOYYFLV12A58A7A88F      1\n",
       "\n",
       "[48373586 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>song_id</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n      <td>SOAKIMP12A8C130995</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n      <td>SOAPDEY12A81C210A9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n      <td>SOBBMDR12A8C13253B</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n      <td>SOBFNSP12AF72A0E22</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b80344d063b5ccb3212f76538f3d9e43d87dca9e</td>\n      <td>SOBFOVM12A58A7D494</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48373581</th>\n      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n      <td>SOUHHHH12AF729E4AF</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>48373582</th>\n      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n      <td>SOUJVIT12A8C1451C1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>48373583</th>\n      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n      <td>SOUSMXX12AB0185C24</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>48373584</th>\n      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n      <td>SOWYSKH12AF72A303A</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>48373585</th>\n      <td>b7815dbb206eb2831ce0fe040d0aa537e2e800f7</td>\n      <td>SOYYFLV12A58A7A88F</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>48373586 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 233
    }
   ],
   "source": [
    "filename = 'data/train_triplets.txt'\n",
    "id_df = pd.read_csv(filename, delimiter='\\t',names=['user_id','song_id','count'])\n",
    "id_df"
   ]
  },
  {
   "source": [
    "Read in dataframe linking song_id to song titles"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-231-a6871d1b8ef7>:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n  tracks_df = pd.read_csv(track_file,delimiter='<SEP>',names=['track_id','song_id','artist','song'])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  track_id             song_id            artist  \\\n",
       "0       TRMMMYQ128F932D901  SOQMMHC12AB0180CB8  Faster Pussy cat   \n",
       "1       TRMMMKD128F425225D  SOVFVAK12A8C1350D9  Karkkiautomaatti   \n",
       "2       TRMMMRX128F93187D9  SOGTUKN12AB017F4F1    Hudson Mohawke   \n",
       "3       TRMMMCH128F425532C  SOBNYVR12A8C13558C       Yerba Brava   \n",
       "4       TRMMMWA128F426B589  SOHSBXH12A8C13B0DF        Der Mystic   \n",
       "...                    ...                 ...               ...   \n",
       "999995  TRYYYUS12903CD2DF0  SOTXAME12AB018F136      Kiko Navarro   \n",
       "999996  TRYYYJO128F426DA37  SOXQYIQ12A8C137FBB     Kuldeep Manak   \n",
       "999997  TRYYYMG128F4260ECA  SOHODZI12A8C137BB3    Gabriel Le Mar   \n",
       "999998  TRYYYDJ128F9310A21  SOLXGOR12A81C21EB7             Elude   \n",
       "999999  TRYYYVU12903CD01E3  SOWXJXQ12AB0189F43             Texta   \n",
       "\n",
       "                                        song  \n",
       "0                               Silent Night  \n",
       "1                                Tanssi vaan  \n",
       "2                          No One Could Ever  \n",
       "3                             Si Vos QuerÃ©s  \n",
       "4                           Tangle Of Aspens  \n",
       "...                                      ...  \n",
       "999995                       O Samba Da Vida  \n",
       "999996                          Jago Chhadeo  \n",
       "999997                               Novemba  \n",
       "999998                               Faraday  \n",
       "999999  Fernweh feat. Sektion KuchikÃ¤schtli  \n",
       "\n",
       "[1000000 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_id</th>\n      <th>song_id</th>\n      <th>artist</th>\n      <th>song</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRMMMYQ128F932D901</td>\n      <td>SOQMMHC12AB0180CB8</td>\n      <td>Faster Pussy cat</td>\n      <td>Silent Night</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRMMMKD128F425225D</td>\n      <td>SOVFVAK12A8C1350D9</td>\n      <td>Karkkiautomaatti</td>\n      <td>Tanssi vaan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRMMMRX128F93187D9</td>\n      <td>SOGTUKN12AB017F4F1</td>\n      <td>Hudson Mohawke</td>\n      <td>No One Could Ever</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRMMMCH128F425532C</td>\n      <td>SOBNYVR12A8C13558C</td>\n      <td>Yerba Brava</td>\n      <td>Si Vos QuerÃ©s</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRMMMWA128F426B589</td>\n      <td>SOHSBXH12A8C13B0DF</td>\n      <td>Der Mystic</td>\n      <td>Tangle Of Aspens</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>999995</th>\n      <td>TRYYYUS12903CD2DF0</td>\n      <td>SOTXAME12AB018F136</td>\n      <td>Kiko Navarro</td>\n      <td>O Samba Da Vida</td>\n    </tr>\n    <tr>\n      <th>999996</th>\n      <td>TRYYYJO128F426DA37</td>\n      <td>SOXQYIQ12A8C137FBB</td>\n      <td>Kuldeep Manak</td>\n      <td>Jago Chhadeo</td>\n    </tr>\n    <tr>\n      <th>999997</th>\n      <td>TRYYYMG128F4260ECA</td>\n      <td>SOHODZI12A8C137BB3</td>\n      <td>Gabriel Le Mar</td>\n      <td>Novemba</td>\n    </tr>\n    <tr>\n      <th>999998</th>\n      <td>TRYYYDJ128F9310A21</td>\n      <td>SOLXGOR12A81C21EB7</td>\n      <td>Elude</td>\n      <td>Faraday</td>\n    </tr>\n    <tr>\n      <th>999999</th>\n      <td>TRYYYVU12903CD01E3</td>\n      <td>SOWXJXQ12AB0189F43</td>\n      <td>Texta</td>\n      <td>Fernweh feat. Sektion KuchikÃ¤schtli</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000000 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 231
    }
   ],
   "source": [
    "track_file = 'data/unique_tracks.txt'\n",
    "tracks_df = pd.read_csv(track_file,delimiter='<SEP>',names=['track_id','song_id','artist','song'])\n",
    "tracks_df"
   ]
  },
  {
   "source": [
    "Now join those two dataframes on the shared song id"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                    track_id             song_id            artist  \\\n",
       "0         TRMMMYQ128F932D901  SOQMMHC12AB0180CB8  Faster Pussy cat   \n",
       "1         TRMMMYQ128F932D901  SOQMMHC12AB0180CB8  Faster Pussy cat   \n",
       "2         TRMMMYQ128F932D901  SOQMMHC12AB0180CB8  Faster Pussy cat   \n",
       "3         TRMMMCH128F425532C  SOBNYVR12A8C13558C       Yerba Brava   \n",
       "4         TRMMMCH128F425532C  SOBNYVR12A8C13558C       Yerba Brava   \n",
       "...                      ...                 ...               ...   \n",
       "49664523  TRYYYZM128F428E804  SOBODSE12A8C13EBD6           SKYCLAD   \n",
       "49664524  TRYYYZM128F428E804  SOBODSE12A8C13EBD6           SKYCLAD   \n",
       "49664525  TRYYYON128F932585A  SOWCNSN12AB018070F        Loose Shus   \n",
       "49664526  TRYYYON128F932585A  SOWCNSN12AB018070F        Loose Shus   \n",
       "49664527  TRYYYVU12903CD01E3  SOWXJXQ12AB0189F43             Texta   \n",
       "\n",
       "                                          song  \\\n",
       "0                                 Silent Night   \n",
       "1                                 Silent Night   \n",
       "2                                 Silent Night   \n",
       "3                               Si Vos QuerÃ©s   \n",
       "4                               Si Vos QuerÃ©s   \n",
       "...                                        ...   \n",
       "49664523                     Inequality Street   \n",
       "49664524                     Inequality Street   \n",
       "49664525              Taurus (Keenhouse Remix)   \n",
       "49664526              Taurus (Keenhouse Remix)   \n",
       "49664527  Fernweh feat. Sektion KuchikÃ¤schtli   \n",
       "\n",
       "                                           user_id  count  \n",
       "0         3a05343210b5e4b6308193bcd00242d326bd9b36      1  \n",
       "1         93f24a7eb6742300414e7b8d4fefddf3f90c3db7      6  \n",
       "2         53f8a04762e391eb0efb812b7352e4d598a48b2c      1  \n",
       "3         baf8f44f7f23ca9671be11ff296df32a09f4406d      1  \n",
       "4         c59ecc2ed2f13812c69a65c02d9847d255fa8ecf      1  \n",
       "...                                            ...    ...  \n",
       "49664523  9ab8ef65e878846e1e2b4e1109ffa56b1a2c09bb      1  \n",
       "49664524  56d29a12dbd87b619256e349d967fef6e1a46698      1  \n",
       "49664525  aeb227fce19da4e5a1d88f53df2ac0eb3ede4f74      1  \n",
       "49664526  95061cbe060d2500ce8af2bc71bcd9a6512352ba      2  \n",
       "49664527  73761dab0fa190edb18a2b21c4cfcf76d7cd8474      1  \n",
       "\n",
       "[49664528 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_id</th>\n      <th>song_id</th>\n      <th>artist</th>\n      <th>song</th>\n      <th>user_id</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRMMMYQ128F932D901</td>\n      <td>SOQMMHC12AB0180CB8</td>\n      <td>Faster Pussy cat</td>\n      <td>Silent Night</td>\n      <td>3a05343210b5e4b6308193bcd00242d326bd9b36</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRMMMYQ128F932D901</td>\n      <td>SOQMMHC12AB0180CB8</td>\n      <td>Faster Pussy cat</td>\n      <td>Silent Night</td>\n      <td>93f24a7eb6742300414e7b8d4fefddf3f90c3db7</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRMMMYQ128F932D901</td>\n      <td>SOQMMHC12AB0180CB8</td>\n      <td>Faster Pussy cat</td>\n      <td>Silent Night</td>\n      <td>53f8a04762e391eb0efb812b7352e4d598a48b2c</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRMMMCH128F425532C</td>\n      <td>SOBNYVR12A8C13558C</td>\n      <td>Yerba Brava</td>\n      <td>Si Vos QuerÃ©s</td>\n      <td>baf8f44f7f23ca9671be11ff296df32a09f4406d</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRMMMCH128F425532C</td>\n      <td>SOBNYVR12A8C13558C</td>\n      <td>Yerba Brava</td>\n      <td>Si Vos QuerÃ©s</td>\n      <td>c59ecc2ed2f13812c69a65c02d9847d255fa8ecf</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49664523</th>\n      <td>TRYYYZM128F428E804</td>\n      <td>SOBODSE12A8C13EBD6</td>\n      <td>SKYCLAD</td>\n      <td>Inequality Street</td>\n      <td>9ab8ef65e878846e1e2b4e1109ffa56b1a2c09bb</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49664524</th>\n      <td>TRYYYZM128F428E804</td>\n      <td>SOBODSE12A8C13EBD6</td>\n      <td>SKYCLAD</td>\n      <td>Inequality Street</td>\n      <td>56d29a12dbd87b619256e349d967fef6e1a46698</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49664525</th>\n      <td>TRYYYON128F932585A</td>\n      <td>SOWCNSN12AB018070F</td>\n      <td>Loose Shus</td>\n      <td>Taurus (Keenhouse Remix)</td>\n      <td>aeb227fce19da4e5a1d88f53df2ac0eb3ede4f74</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49664526</th>\n      <td>TRYYYON128F932585A</td>\n      <td>SOWCNSN12AB018070F</td>\n      <td>Loose Shus</td>\n      <td>Taurus (Keenhouse Remix)</td>\n      <td>95061cbe060d2500ce8af2bc71bcd9a6512352ba</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>49664527</th>\n      <td>TRYYYVU12903CD01E3</td>\n      <td>SOWXJXQ12AB0189F43</td>\n      <td>Texta</td>\n      <td>Fernweh feat. Sektion KuchikÃ¤schtli</td>\n      <td>73761dab0fa190edb18a2b21c4cfcf76d7cd8474</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>49664528 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 229
    }
   ],
   "source": [
    "df = pd.merge(tracks_df, id_df, how='inner', on='song_id')\n",
    "df"
   ]
  },
  {
   "source": [
    "Now we read in our lyric data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             Band                                             Lyrics  \\\n",
       "0    Elijah Blake  No, no\\r\\nI ain't ever trapped out the bando\\r...   \n",
       "1    Elijah Blake  The drinks go down and smoke goes up, I feel m...   \n",
       "2    Elijah Blake  She don't live on planet Earth no more\\r\\nShe ...   \n",
       "3    Elijah Blake  Trippin' off that Grigio, mobbin', lights low\\...   \n",
       "4    Elijah Blake  I see a midnight panther, so gallant and so br...   \n",
       "..            ...                                                ...   \n",
       "994  Judy Garland  Day in, day out\\r\\nThe same old hoodoo follows...   \n",
       "995  Judy Garland  When a dove is in love\\r\\nWith a doll of a dov...   \n",
       "996  Judy Garland  You've got me where you want me\\r\\nAnd I hope ...   \n",
       "997  Judy Garland  If you're ever in a jam, here I am \\r\\nIf you'...   \n",
       "998  Judy Garland  Judy\\r\\nForget your troubles\\r\\n\\r\\nBarbra\\r\\n...   \n",
       "\n",
       "                                                Song  original_index  \n",
       "0                                           Everyday               0  \n",
       "1                                   Live Till We Die               1  \n",
       "2                                      The Otherside               2  \n",
       "3                                              Pinot               3  \n",
       "4                                 Shadows & Diamonds               4  \n",
       "..                                               ...             ...  \n",
       "994                                  Day In, Day Out             994  \n",
       "995  I Could Go on Singin' ('Til the Cows Come Home)             995  \n",
       "996                     You Got Me Where You Want Me             996  \n",
       "997                              Friendship (Medley)             997  \n",
       "998            Get Happy - Happy Days Are Here Again             998  \n",
       "\n",
       "[999 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Band</th>\n      <th>Lyrics</th>\n      <th>Song</th>\n      <th>original_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Elijah Blake</td>\n      <td>No, no\\r\\nI ain't ever trapped out the bando\\r...</td>\n      <td>Everyday</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Elijah Blake</td>\n      <td>The drinks go down and smoke goes up, I feel m...</td>\n      <td>Live Till We Die</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Elijah Blake</td>\n      <td>She don't live on planet Earth no more\\r\\nShe ...</td>\n      <td>The Otherside</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Elijah Blake</td>\n      <td>Trippin' off that Grigio, mobbin', lights low\\...</td>\n      <td>Pinot</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Elijah Blake</td>\n      <td>I see a midnight panther, so gallant and so br...</td>\n      <td>Shadows &amp; Diamonds</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>994</th>\n      <td>Judy Garland</td>\n      <td>Day in, day out\\r\\nThe same old hoodoo follows...</td>\n      <td>Day In, Day Out</td>\n      <td>994</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>Judy Garland</td>\n      <td>When a dove is in love\\r\\nWith a doll of a dov...</td>\n      <td>I Could Go on Singin' ('Til the Cows Come Home)</td>\n      <td>995</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>Judy Garland</td>\n      <td>You've got me where you want me\\r\\nAnd I hope ...</td>\n      <td>You Got Me Where You Want Me</td>\n      <td>996</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>Judy Garland</td>\n      <td>If you're ever in a jam, here I am \\r\\nIf you'...</td>\n      <td>Friendship (Medley)</td>\n      <td>997</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>Judy Garland</td>\n      <td>Judy\\r\\nForget your troubles\\r\\n\\r\\nBarbra\\r\\n...</td>\n      <td>Get Happy - Happy Days Are Here Again</td>\n      <td>998</td>\n    </tr>\n  </tbody>\n</table>\n<p>999 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 424
    }
   ],
   "source": [
    "ourfile = 'data/first1000.csv'\n",
    "\n",
    "lyric_df = pd.read_csv(ourfile)\n",
    "lyric_df['original_index'] = lyric_df.index\n",
    "# this is needed incase we want to map back to the original index in the lyric table after all the joins\n",
    "lyric_df"
   ]
  },
  {
   "source": [
    "Now we join our data all together by matching on both artist and song name - this helps filter out duplicate song names, and there were many!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                track_id             song_id           artist      song  \\\n",
       "0     TRMHWSF128F934D22D  SOIBNKE12AB0181BB6  Eliza Doolittle   Go Home   \n",
       "1     TRMHWSF128F934D22D  SOIBNKE12AB0181BB6  Eliza Doolittle   Go Home   \n",
       "2     TRMHWSF128F934D22D  SOIBNKE12AB0181BB6  Eliza Doolittle   Go Home   \n",
       "3     TRMHWSF128F934D22D  SOIBNKE12AB0181BB6  Eliza Doolittle   Go Home   \n",
       "4     TRMHWSF128F934D22D  SOIBNKE12AB0181BB6  Eliza Doolittle   Go Home   \n",
       "...                  ...                 ...              ...       ...   \n",
       "4824  TRYIHNA128F934D221  SOZKBKD12AB0181B90  Eliza Doolittle  Moneybox   \n",
       "4825  TRYIHNA128F934D221  SOZKBKD12AB0181B90  Eliza Doolittle  Moneybox   \n",
       "4826  TRYIHNA128F934D221  SOZKBKD12AB0181B90  Eliza Doolittle  Moneybox   \n",
       "4827  TRYIHNA128F934D221  SOZKBKD12AB0181B90  Eliza Doolittle  Moneybox   \n",
       "4828  TRYIHNA128F934D221  SOZKBKD12AB0181B90  Eliza Doolittle  Moneybox   \n",
       "\n",
       "                                       user_id  count             Band  \\\n",
       "0     4a6bf06cffacc38b51d598940c06dc35f0685c01      3  Eliza Doolittle   \n",
       "1     bbb1f39b74a5974230c4e3343b902f382883175c      2  Eliza Doolittle   \n",
       "2     756d56c83ea2003ac3d8e4b30ddc51e02e52e5cf      1  Eliza Doolittle   \n",
       "3     63e5f42dd18a5f5b1d2df3fecebe6a5ecfc35a27      4  Eliza Doolittle   \n",
       "4     19cdd2de23e456fdfca3f7dc68aa6601765f2db3      1  Eliza Doolittle   \n",
       "...                                        ...    ...              ...   \n",
       "4824  5e9550f4c7a747283194da37d27a5456e2b6d408      4  Eliza Doolittle   \n",
       "4825  db8a0ee12ad49667d5f1ae6793672e10b97de7a6      1  Eliza Doolittle   \n",
       "4826  5e383bf949bc6eee6be0b3db6f08608270352f9b      1  Eliza Doolittle   \n",
       "4827  5b62dacbf67eb61043206d8c1edda6b65f37f11f      1  Eliza Doolittle   \n",
       "4828  d9353891b168006a0af564d80aedcea449c0ec3d      4  Eliza Doolittle   \n",
       "\n",
       "                                                 Lyrics      Song  \\\n",
       "0     Peepin' out the door\\nI see those choppers are...   Go Home   \n",
       "1     Peepin' out the door\\nI see those choppers are...   Go Home   \n",
       "2     Peepin' out the door\\nI see those choppers are...   Go Home   \n",
       "3     Peepin' out the door\\nI see those choppers are...   Go Home   \n",
       "4     Peepin' out the door\\nI see those choppers are...   Go Home   \n",
       "...                                                 ...       ...   \n",
       "4824  Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...  Moneybox   \n",
       "4825  Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...  Moneybox   \n",
       "4826  Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...  Moneybox   \n",
       "4827  Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...  Moneybox   \n",
       "4828  Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...  Moneybox   \n",
       "\n",
       "      original_index  \n",
       "0                187  \n",
       "1                187  \n",
       "2                187  \n",
       "3                187  \n",
       "4                187  \n",
       "...              ...  \n",
       "4824             189  \n",
       "4825             189  \n",
       "4826             189  \n",
       "4827             189  \n",
       "4828             189  \n",
       "\n",
       "[4829 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_id</th>\n      <th>song_id</th>\n      <th>artist</th>\n      <th>song</th>\n      <th>user_id</th>\n      <th>count</th>\n      <th>Band</th>\n      <th>Lyrics</th>\n      <th>Song</th>\n      <th>original_index</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRMHWSF128F934D22D</td>\n      <td>SOIBNKE12AB0181BB6</td>\n      <td>Eliza Doolittle</td>\n      <td>Go Home</td>\n      <td>4a6bf06cffacc38b51d598940c06dc35f0685c01</td>\n      <td>3</td>\n      <td>Eliza Doolittle</td>\n      <td>Peepin' out the door\\nI see those choppers are...</td>\n      <td>Go Home</td>\n      <td>187</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRMHWSF128F934D22D</td>\n      <td>SOIBNKE12AB0181BB6</td>\n      <td>Eliza Doolittle</td>\n      <td>Go Home</td>\n      <td>bbb1f39b74a5974230c4e3343b902f382883175c</td>\n      <td>2</td>\n      <td>Eliza Doolittle</td>\n      <td>Peepin' out the door\\nI see those choppers are...</td>\n      <td>Go Home</td>\n      <td>187</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRMHWSF128F934D22D</td>\n      <td>SOIBNKE12AB0181BB6</td>\n      <td>Eliza Doolittle</td>\n      <td>Go Home</td>\n      <td>756d56c83ea2003ac3d8e4b30ddc51e02e52e5cf</td>\n      <td>1</td>\n      <td>Eliza Doolittle</td>\n      <td>Peepin' out the door\\nI see those choppers are...</td>\n      <td>Go Home</td>\n      <td>187</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRMHWSF128F934D22D</td>\n      <td>SOIBNKE12AB0181BB6</td>\n      <td>Eliza Doolittle</td>\n      <td>Go Home</td>\n      <td>63e5f42dd18a5f5b1d2df3fecebe6a5ecfc35a27</td>\n      <td>4</td>\n      <td>Eliza Doolittle</td>\n      <td>Peepin' out the door\\nI see those choppers are...</td>\n      <td>Go Home</td>\n      <td>187</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRMHWSF128F934D22D</td>\n      <td>SOIBNKE12AB0181BB6</td>\n      <td>Eliza Doolittle</td>\n      <td>Go Home</td>\n      <td>19cdd2de23e456fdfca3f7dc68aa6601765f2db3</td>\n      <td>1</td>\n      <td>Eliza Doolittle</td>\n      <td>Peepin' out the door\\nI see those choppers are...</td>\n      <td>Go Home</td>\n      <td>187</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4824</th>\n      <td>TRYIHNA128F934D221</td>\n      <td>SOZKBKD12AB0181B90</td>\n      <td>Eliza Doolittle</td>\n      <td>Moneybox</td>\n      <td>5e9550f4c7a747283194da37d27a5456e2b6d408</td>\n      <td>4</td>\n      <td>Eliza Doolittle</td>\n      <td>Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...</td>\n      <td>Moneybox</td>\n      <td>189</td>\n    </tr>\n    <tr>\n      <th>4825</th>\n      <td>TRYIHNA128F934D221</td>\n      <td>SOZKBKD12AB0181B90</td>\n      <td>Eliza Doolittle</td>\n      <td>Moneybox</td>\n      <td>db8a0ee12ad49667d5f1ae6793672e10b97de7a6</td>\n      <td>1</td>\n      <td>Eliza Doolittle</td>\n      <td>Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...</td>\n      <td>Moneybox</td>\n      <td>189</td>\n    </tr>\n    <tr>\n      <th>4826</th>\n      <td>TRYIHNA128F934D221</td>\n      <td>SOZKBKD12AB0181B90</td>\n      <td>Eliza Doolittle</td>\n      <td>Moneybox</td>\n      <td>5e383bf949bc6eee6be0b3db6f08608270352f9b</td>\n      <td>1</td>\n      <td>Eliza Doolittle</td>\n      <td>Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...</td>\n      <td>Moneybox</td>\n      <td>189</td>\n    </tr>\n    <tr>\n      <th>4827</th>\n      <td>TRYIHNA128F934D221</td>\n      <td>SOZKBKD12AB0181B90</td>\n      <td>Eliza Doolittle</td>\n      <td>Moneybox</td>\n      <td>5b62dacbf67eb61043206d8c1edda6b65f37f11f</td>\n      <td>1</td>\n      <td>Eliza Doolittle</td>\n      <td>Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...</td>\n      <td>Moneybox</td>\n      <td>189</td>\n    </tr>\n    <tr>\n      <th>4828</th>\n      <td>TRYIHNA128F934D221</td>\n      <td>SOZKBKD12AB0181B90</td>\n      <td>Eliza Doolittle</td>\n      <td>Moneybox</td>\n      <td>d9353891b168006a0af564d80aedcea449c0ec3d</td>\n      <td>4</td>\n      <td>Eliza Doolittle</td>\n      <td>Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...</td>\n      <td>Moneybox</td>\n      <td>189</td>\n    </tr>\n  </tbody>\n</table>\n<p>4829 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 313
    }
   ],
   "source": [
    "df = pd.merge(df, lyric_df, how='inner',left_on=['song','artist'],right_on=['Song','Band'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unique users:  2855\nunique songs:  34\n"
     ]
    }
   ],
   "source": [
    "print(\"unique users: \", df['user_id'].unique().size)\n",
    "print(\"unique songs: \", df['song_id'].unique().size)"
   ]
  },
  {
   "source": [
    "This step allows us to easily re-index our song id's into an easy to use form. We map song_id -> integer value that can be used to index our matrices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_SongID = df['song_id'].unique()\n",
    "unique_UserID = df['user_id'].unique()\n",
    "j = 0\n",
    "user_old2new_id_dict = dict()\n",
    "for u in unique_UserID:\n",
    "    user_old2new_id_dict[u] = j\n",
    "    j += 1\n",
    "j = 0\n",
    "song_old2new_id_dict = dict() \n",
    "for i in unique_SongID:\n",
    "    song_old2new_id_dict[i] = j\n",
    "    j += 1\n",
    "    \n",
    "# Then, use the generated dictionaries to reindex UserID and Song in the df\n",
    "for j in range(len(df)):\n",
    "    df.at[j, 'user_id'] = user_old2new_id_dict[df.at[j, 'user_id']]\n",
    "    df.at[j, 'song_id'] = song_old2new_id_dict[df.at[j, 'song_id']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                track_id song_id           artist      song user_id  count  \\\n",
       "0     TRMHWSF128F934D22D       0  Eliza Doolittle   Go Home       0      3   \n",
       "1     TRMHWSF128F934D22D       0  Eliza Doolittle   Go Home       1      2   \n",
       "2     TRMHWSF128F934D22D       0  Eliza Doolittle   Go Home       2      1   \n",
       "3     TRMHWSF128F934D22D       0  Eliza Doolittle   Go Home       3      4   \n",
       "4     TRMHWSF128F934D22D       0  Eliza Doolittle   Go Home       4      1   \n",
       "...                  ...     ...              ...       ...     ...    ...   \n",
       "4824  TRYIHNA128F934D221      33  Eliza Doolittle  Moneybox     542      4   \n",
       "4825  TRYIHNA128F934D221      33  Eliza Doolittle  Moneybox     544      1   \n",
       "4826  TRYIHNA128F934D221      33  Eliza Doolittle  Moneybox     770      1   \n",
       "4827  TRYIHNA128F934D221      33  Eliza Doolittle  Moneybox    2760      1   \n",
       "4828  TRYIHNA128F934D221      33  Eliza Doolittle  Moneybox    2854      4   \n",
       "\n",
       "                 Band                                             Lyrics  \\\n",
       "0     Eliza Doolittle  Peepin' out the door\\nI see those choppers are...   \n",
       "1     Eliza Doolittle  Peepin' out the door\\nI see those choppers are...   \n",
       "2     Eliza Doolittle  Peepin' out the door\\nI see those choppers are...   \n",
       "3     Eliza Doolittle  Peepin' out the door\\nI see those choppers are...   \n",
       "4     Eliza Doolittle  Peepin' out the door\\nI see those choppers are...   \n",
       "...               ...                                                ...   \n",
       "4824  Eliza Doolittle  Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...   \n",
       "4825  Eliza Doolittle  Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...   \n",
       "4826  Eliza Doolittle  Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...   \n",
       "4827  Eliza Doolittle  Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...   \n",
       "4828  Eliza Doolittle  Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...   \n",
       "\n",
       "          Song  \n",
       "0      Go Home  \n",
       "1      Go Home  \n",
       "2      Go Home  \n",
       "3      Go Home  \n",
       "4      Go Home  \n",
       "...        ...  \n",
       "4824  Moneybox  \n",
       "4825  Moneybox  \n",
       "4826  Moneybox  \n",
       "4827  Moneybox  \n",
       "4828  Moneybox  \n",
       "\n",
       "[4829 rows x 9 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>track_id</th>\n      <th>song_id</th>\n      <th>artist</th>\n      <th>song</th>\n      <th>user_id</th>\n      <th>count</th>\n      <th>Band</th>\n      <th>Lyrics</th>\n      <th>Song</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TRMHWSF128F934D22D</td>\n      <td>0</td>\n      <td>Eliza Doolittle</td>\n      <td>Go Home</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Eliza Doolittle</td>\n      <td>Peepin' out the door\\nI see those choppers are...</td>\n      <td>Go Home</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TRMHWSF128F934D22D</td>\n      <td>0</td>\n      <td>Eliza Doolittle</td>\n      <td>Go Home</td>\n      <td>1</td>\n      <td>2</td>\n      <td>Eliza Doolittle</td>\n      <td>Peepin' out the door\\nI see those choppers are...</td>\n      <td>Go Home</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TRMHWSF128F934D22D</td>\n      <td>0</td>\n      <td>Eliza Doolittle</td>\n      <td>Go Home</td>\n      <td>2</td>\n      <td>1</td>\n      <td>Eliza Doolittle</td>\n      <td>Peepin' out the door\\nI see those choppers are...</td>\n      <td>Go Home</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TRMHWSF128F934D22D</td>\n      <td>0</td>\n      <td>Eliza Doolittle</td>\n      <td>Go Home</td>\n      <td>3</td>\n      <td>4</td>\n      <td>Eliza Doolittle</td>\n      <td>Peepin' out the door\\nI see those choppers are...</td>\n      <td>Go Home</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TRMHWSF128F934D22D</td>\n      <td>0</td>\n      <td>Eliza Doolittle</td>\n      <td>Go Home</td>\n      <td>4</td>\n      <td>1</td>\n      <td>Eliza Doolittle</td>\n      <td>Peepin' out the door\\nI see those choppers are...</td>\n      <td>Go Home</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4824</th>\n      <td>TRYIHNA128F934D221</td>\n      <td>33</td>\n      <td>Eliza Doolittle</td>\n      <td>Moneybox</td>\n      <td>542</td>\n      <td>4</td>\n      <td>Eliza Doolittle</td>\n      <td>Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...</td>\n      <td>Moneybox</td>\n    </tr>\n    <tr>\n      <th>4825</th>\n      <td>TRYIHNA128F934D221</td>\n      <td>33</td>\n      <td>Eliza Doolittle</td>\n      <td>Moneybox</td>\n      <td>544</td>\n      <td>1</td>\n      <td>Eliza Doolittle</td>\n      <td>Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...</td>\n      <td>Moneybox</td>\n    </tr>\n    <tr>\n      <th>4826</th>\n      <td>TRYIHNA128F934D221</td>\n      <td>33</td>\n      <td>Eliza Doolittle</td>\n      <td>Moneybox</td>\n      <td>770</td>\n      <td>1</td>\n      <td>Eliza Doolittle</td>\n      <td>Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...</td>\n      <td>Moneybox</td>\n    </tr>\n    <tr>\n      <th>4827</th>\n      <td>TRYIHNA128F934D221</td>\n      <td>33</td>\n      <td>Eliza Doolittle</td>\n      <td>Moneybox</td>\n      <td>2760</td>\n      <td>1</td>\n      <td>Eliza Doolittle</td>\n      <td>Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...</td>\n      <td>Moneybox</td>\n    </tr>\n    <tr>\n      <th>4828</th>\n      <td>TRYIHNA128F934D221</td>\n      <td>33</td>\n      <td>Eliza Doolittle</td>\n      <td>Moneybox</td>\n      <td>2854</td>\n      <td>4</td>\n      <td>Eliza Doolittle</td>\n      <td>Instead of goin' out\\r\\nTo dinner tonight\\r\\nW...</td>\n      <td>Moneybox</td>\n    </tr>\n  </tbody>\n</table>\n<p>4829 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 166
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "source": [
    "# Part 2: Create Train & Test data\n",
    "\n",
    "Originally, we wanted the matrices we work with to be of dimension (users, songs), where each entry (u,s) is the number of times the user has listened to that song. However, we realized it is better to fill the matrices with the *fraction* of total listens for that particular user. This way, the model is agnostic of different users listening at differing rates.  \n",
    "\n",
    "First we sample our df to create our train and test data, and check for overlap\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "30420\n13041\nno overlap!\n"
     ]
    }
   ],
   "source": [
    "train_df = df.sample(frac=0.7, random_state=0)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# fill nan values\n",
    "train_mat[np.isnan(train_mat)] = 0\n",
    "test_mat[np.isnan(test_mat)] = 0\n",
    "\n",
    "print(train_df.size)\n",
    "print(test_df.size)\n",
    "\n",
    "if(train_df.size + test_df.size == df.size):\n",
    "    print(\"no overlap!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2855, 34)\n(2855, 34)\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "num_user = len(df['user_id'].unique())\n",
    "num_song = len(df['song_id'].unique())\n",
    "\n",
    "train_mat = coo_matrix((train_df['count'].values, (train_df['user_id'].values, train_df['song_id'].values)), shape=(num_user, num_song)).toarray().astype(float)\n",
    "test_mat = coo_matrix((test_df['count'].values, (test_df['user_id'].values, test_df['song_id'].values)), shape=(num_user, num_song)).toarray().astype(float)\n",
    "\n",
    "print(train_mat.shape)\n",
    "print(test_mat.shape)"
   ]
  },
  {
   "source": [
    "Standardize the user counts, so every song listen value becomes a percentage of user listens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat_counts = np.sum(train_mat,axis=1).T\n",
    "test_mat_counts = np.sum(test_mat,axis=1).T\n",
    "\n",
    "for user_id in range(num_user):\n",
    "    \n",
    "    denom = 1\n",
    "    if train_mat_counts[user_id] != 0:\n",
    "        denom = train_mat_counts[user_id]\n",
    "\n",
    "    train_mat[user_id] = train_mat[user_id]/denom\n",
    "    test_mat[user_id] = test_mat[user_id]/denom"
   ]
  },
  {
   "source": [
    "# Part 3: Model Creation\n",
    "\n",
    "We will create and validate 3 models - Bag of Words, Bert, and a user-user MF model. All of these models will essentially generate latent features in some way given our (user, song) train data, and then generate predictions for fraction of listens. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 3a: Bag of Words\n",
    "\n",
    "Our simple bag of words model has a few steps:\n",
    "\n",
    "1. For all lyrics, count the top N most frequent, nontrivial terms\n",
    "2. For each song, generate an N-length vector containing counts of those terms\n",
    "3. For each user, generate a user profile songs weighted by listens\n",
    "4. Produce a prediction for each user-song pair with the product of the user profile and song "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Aditya\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import pairwise\n",
    "\n",
    "\n",
    "BAG_SIZE = 1000\n",
    "def get_top_values(d,N = BAG_SIZE):\n",
    "    return list(sorted(d.items(), key = itemgetter(1), reverse = True)[:N])\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# creating  \"master\" dictionary\n",
    "word2count = {}\n",
    "\n",
    "for song_id in range(num_song):\n",
    "\n",
    "    # this will get the first index where song_id appears\n",
    "    lyric_idx = df[df['song_id']==song_id].index.values[0]\n",
    "    \n",
    "    # get song lyrics at that index\n",
    "    song_lyric = df.Lyrics.iloc[lyric_idx]\n",
    "\n",
    "    # clean song lyrics\n",
    "    song_lyric = song_lyric.lower()\n",
    "    song_lyric = re.sub(r'\\W', ' ', song_lyric)\n",
    "    song_lyic = re.sub(r'\\s+', ' ', song_lyric)\n",
    "    song_lyric = re.sub(r'\\r|\\n', ' ', song_lyric)\n",
    "\n",
    "    # get word counts for songs omitting stopwords\n",
    "    words = nltk.word_tokenize(song_lyric)\n",
    "    for word in words:\n",
    "\n",
    "        if word not in stop_words:\n",
    "\n",
    "            if word not in word2count:\n",
    "                word2count[word] = 1\n",
    "            else:\n",
    "                word2count[word] += 1\n",
    "\n",
    "# get the top ranked words for our songs\n",
    "\n",
    "word2count_list = get_top_values(word2count)\n",
    "word2count = dict(word2count_list)\n",
    "word_ranks = {w2c[0]: r for r, w2c in enumerate(word2count_list)}\n",
    "\n",
    "# Now pass through again and create vectors\n",
    "\n",
    "song_counts = np.zeros([num_song, BAG_SIZE])\n",
    "\n",
    "for song_id in range(num_song):\n",
    "\n",
    "    lyric_idx = df[df['song_id']==song_id].index.values[0]\n",
    "    \n",
    "    # get song lyrics at that index\n",
    "    song_lyric = df.Lyrics.iloc[lyric_idx]\n",
    "\n",
    "    # clean song lyrics\n",
    "    song_lyric = song_lyric.lower()\n",
    "    song_lyric = re.sub(r'\\W', ' ', song_lyric)\n",
    "    song_lyic = re.sub(r'\\s+', ' ', song_lyric)\n",
    "    song_lyric = re.sub(r'\\r|\\n', ' ', song_lyric)\n",
    "\n",
    "    words = nltk.word_tokenize(song_lyric)\n",
    "\n",
    "    for word in words:\n",
    "        if word in word_ranks:\n",
    "            word_idx = word_ranks[word]\n",
    "            song_counts[song_id][word_idx] += 1\n",
    "\n",
    "song_pairwise_similarities = pairwise.cosine_similarity(song_counts)"
   ]
  },
  {
   "source": [
    "\n",
    "Generate a user profile based on the tastes of each user. This will be a weighted average  \n",
    "of all the songs the user has listened to, based on relative number of listens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_user_profiles =  np.zeros([num_user, num_bert])\n",
    "\n",
    "for user_id in range(num_user):\n",
    "\n",
    "    user_listens = train_mat[user_id]\n",
    "\n",
    "    for song_id in range(num_song):\n",
    "\n",
    "        # add song vectors weighted by listens to user profile\n",
    "        listen_count = train_mat[user_id][song_id]\n",
    "        bow_user_profiles[user_id] += listen_count*song_counts[song_id]\n"
   ]
  },
  {
   "source": [
    "Create predictions for bag of words by getting the product of user profiles with songs. Ignore users with no listens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "406\n"
     ]
    }
   ],
   "source": [
    "bow_prediction_mat = user_profiles @ song_counts.T\n",
    "bow_prediction_mat_counts = np.sum(bow_prediction_mat,axis=1).T\n",
    "\n",
    "for user_id in range(num_user):\n",
    "    \n",
    "    denom = 1\n",
    "    if bow_prediction_mat_counts[user_id] != 0:\n",
    "        denom = bow_prediction_mat_counts[user_id]\n",
    "\n",
    "    bow_prediction_mat[user_id] = bow_prediction_mat[user_id]/denom\n",
    "\n",
    "bow_prediction_mat"
   ]
  },
  {
   "source": [
    "##3b: BERT\n",
    "\n",
    "Our BERT model is similarly structured, except how we get our latent features\n",
    "\n",
    "1. Compute latent features for lyrics by feeding into BERT, a pretrained model \n",
    "2. For each user, generate a user profile that is weighted by listens. So each user profile should be as long as the latent dimension\n",
    "3. Produce a prediction for each user-song pair with the product of the user profile and latent representation of the song\n",
    "\n",
    "First however, we have to read in our document embeddings that we computed earlier and wrote to a file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frac = 1.0\n",
    "filepath = f\"data/lyrec_embeddings_{sample_frac}.pkl.npy\"\n",
    "\n",
    "if os.path.isfile(filepath):\n",
    "    got_from_file = True\n",
    "    document_embeddings = np.load(filepath)\n",
    "else:\n",
    "    print('no file path')\n",
    "\n",
    "if not got_from_file:\n",
    "    np.save(filepath, document_embeddings)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(516174, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 308
    }
   ],
   "source": [
    "document_embeddings.shape"
   ]
  },
  {
   "source": [
    "Now we use the original index to map back to the correct lyrics, since BERT computed many redundant lyric representations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(516174, 768)\n",
      "(4829, 768)\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise\n",
    "print(document_embeddings.shape)\n",
    "\n",
    "subset_embeddings = document_embeddings[df.original_index]\n",
    "print(subset_embeddings.shape)\n",
    "\n",
    "print(len(np.unique(subset_embeddings, axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(34, 768)"
      ]
     },
     "metadata": {},
     "execution_count": 342
    }
   ],
   "source": [
    "num_bert = subset_embeddings.shape[1]\n",
    "bert_factors = np.zeros([num_song, num_bert])\n",
    "\n",
    "for song_id in range(num_song):\n",
    "\n",
    "    # get the original index where the first song id appears in the df\n",
    "    original_idx = df[df['song_id']==song_id].original_index.values[0]\n",
    "    # get song lyrics at that index\n",
    "    bert_factors[song_id] = subset_embeddings[original_idx]\n",
    "\n",
    "bert_factors.shape"
   ]
  },
  {
   "source": [
    "Compute bert user profiles as the weighted average of latent representations of songs, weighted by the listens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "114\n"
     ]
    }
   ],
   "source": [
    "bert_user_profiles =  np.zeros([num_user, num_bert])\n",
    "\n",
    "for user_id in range(num_user):\n",
    "\n",
    "    user_listens = train_mat[user_id]\n",
    "\n",
    "    for song_id in range(num_song):\n",
    "\n",
    "        # add song vectors weighted by listens to user profile\n",
    "        listen_count = train_mat[user_id][song_id]\n",
    "        bert_user_profiles[user_id] += listen_count*bert_factors[song_id]"
   ]
  },
  {
   "source": [
    "Get product of user profiles and song representations for predicted values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2855\n24\n"
     ]
    }
   ],
   "source": [
    "bert_prediction_mat = bert_user_profiles @ bert_factors.T\n",
    "bert_prediction_mat_counts = np.sum(bert_prediction_mat,axis=1).T\n",
    "\n",
    "for user_id in range(num_user):\n",
    "    \n",
    "    denom = 1\n",
    "    if bert_prediction_mat_counts[user_id] != 0:\n",
    "        denom = bert_prediction_mat_counts[user_id]\n",
    "\n",
    "    bert_prediction_mat[user_id] = bert_prediction_mat[user_id]/denom\n",
    "\n",
    "bert_prediction_mat"
   ]
  },
  {
   "source": [
    "# Part 3: User-User MF\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf = NMF()\n",
    "W = nmf.fit_transform(train_mat)\n",
    "H = nmf.components_\n",
    "mf_mat = np.dot(W,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[  60.9 ,  121.05,   28.4 , ...,   23.5 ,  220.3 ,  510.25],\n",
       "       [ 224.6 ,  238.8 ,  119.2 , ...,   27.  ,   58.4 ,   39.8 ],\n",
       "       [ 202.4 ,   17.8 ,   78.6 , ...,   17.  ,  360.  ,  114.2 ],\n",
       "       ...,\n",
       "       [   0.  ,    0.  ,    0.  , ...,    0.  ,    0.  ,    0.  ],\n",
       "       [  35.  ,   43.  ,   38.  , ...,   45.  ,  134.  , 1274.  ],\n",
       "       [  35.  ,   43.  ,   38.  , ...,   45.  ,  134.  , 1274.  ]])"
      ]
     },
     "metadata": {},
     "execution_count": 416
    }
   ],
   "source": [
    "bow_prediction_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_rmse(test_mat,pred_mat):\n",
    "\n",
    "    total = 0\n",
    "    n = pred_mat.size\n",
    "    unrated = 0\n",
    "\n",
    "    for user_id in range(num_user):\n",
    "\n",
    "        if np.sum(pred_mat[user_id])==0:\n",
    "            unrated += num_song\n",
    "        else:\n",
    "            total += np.sum(np.square(test_mat[user_id] - pred_mat[user_id]))\n",
    "    \n",
    "    return np.sqrt(total/(n-unrated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7120218344605813\n0.18755203340460716\n0.12378539712372252\n0.10575903232384942\n"
     ]
    }
   ],
   "source": [
    "print(new_rmse(test_mat,np.random.rand(num_user,num_song)))\n",
    "print(new_rmse(test_mat,mf_mat))\n",
    "print(new_rmse(test_mat,bow_prediction_mat))\n",
    "print(new_rmse(test_mat,bert_prediction_mat))"
   ]
  },
  {
   "source": [
    "## Now compare precision and recall"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_user_profile_similarities = pairwise.cosine_similarity(bow_user_profiles, song_counts)\n",
    "bow_user_profile_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_user_profiles =  np.zeros([num_user, BAG_SIZE])\n",
    "\n",
    "for user_id in range(num_user):\n",
    "\n",
    "    user_listens = train_mat[user_id]\n",
    "\n",
    "    for song_id in range(num_song):\n",
    "\n",
    "        # add song vectors weighted by listens to user profile\n",
    "        listen_count = train_mat[user_id][song_id]\n",
    "        bow_user_profiles[user_id] += listen_count*song_counts[song_id]\n",
    "    \n",
    "#user_profiles\n",
    "#len(np.unique(user_profiles, axis=0))\n",
    "bow_user_profiles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation = np.zeros([num_user, num_song])\n",
    "\n",
    "for user_id in range(num_user):\n",
    "    recommendation[user_id] = np.argsort(user_profile_similarities[user_id])[::-1]\n",
    "\n",
    "recommendation = recommendation.astype(int)\n",
    "recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recall_k(user_id, k):\n",
    "\n",
    "    num_relevant = 0\n",
    "\n",
    "    for i in range(k):\n",
    "\n",
    "        # get top k movie ids for each user\n",
    "        song_id = recommendation[user_id][i]\n",
    "\n",
    "        # count all relevant values that are in top k\n",
    "        if test_mat[user_id][song_id] == 1:\n",
    "            num_relevant += 1\n",
    "    \n",
    "    # count all relevant values\n",
    "    total_relevant = np.count_nonzero(test_mat[user_id]>=1)\n",
    "\n",
    "    return num_relevant/total_relevant\n",
    "\n",
    "def precision_k(user_id, k):\n",
    "\n",
    "    num_relevant = 0\n",
    "\n",
    "    for rank in range(k):\n",
    "\n",
    "        # get top k movie ids for each user\n",
    "        song_id = recommendation[user_id][rank]\n",
    "\n",
    "        # count all relevant values in top k\n",
    "        if test_mat[user_id][song_id] >= 1:\n",
    "            num_relevant += 1\n",
    "    \n",
    "    return num_relevant/k\n",
    "\n",
    "\n",
    "validation_df = {}\n",
    "\n",
    "for k in [5,20,34]:\n",
    "\n",
    "        # make empty lists to contain precision and recal scores\n",
    "        recall_list = []\n",
    "        precision_list = []\n",
    "\n",
    "        for user in range(num_user):\n",
    "\n",
    "            # check that a user in test_mat does not have all nan values\n",
    "            if np.all(test_mat[user]==0) == False:\n",
    "\n",
    "                # add recall and precision scores to list\n",
    "                recall_list.append(recall_k(user,k))\n",
    "                precision_list.append(precision_k(user,k))\n",
    "\n",
    "        # add lists to dictionary\n",
    "        validation_df[f\"recall_{k}\"] = recall_list\n",
    "        validation_df[f\"precision_{k}\"] = precision_list\n",
    "\n",
    "validation_df = pd.DataFrame(validation_df)\n",
    "validation_df.mean(axis=0)"
   ]
  }
 ]
}